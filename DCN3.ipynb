{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Coattention Network (DCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import xavier_initializer\n",
    "from tqdm import trange\n",
    "import logging\n",
    "import matplotlib as mpl\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "self_FLAGS = tf.flags.FLAGS\n",
    "for name in list(self_FLAGS):\n",
    "    delattr(self_FLAGS, name)\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "tf.app.flags.DEFINE_string(\"model\", \"DCN\", \"Choose which model to use baseline/DCN\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate.\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 64, \"Batch size to use during training.\")\n",
    "tf.app.flags.DEFINE_integer(\"epochs\", 2, \"Number of epochs to train.\")\n",
    "tf.app.flags.DEFINE_integer(\"rnn_state_size\", 200, \"Size of RNNs used in the model.\")\n",
    "tf.app.flags.DEFINE_string(\"figure_directory\", \"figs/\", \"Directory in which figures are stored.\")\n",
    "tf.app.flags.DEFINE_integer(\"word_vec_dim\", 100, \"Dimension of word vectors. Either 100 or 300\")\n",
    "tf.app.flags.DEFINE_float(\"dropout\", 0.6, \"1-Fraction of units randomly dropped.\")\n",
    "tf.app.flags.DEFINE_float(\"dropout_encoder\", 0.7, \"1-Fraction of units randomly dropped in the encoder.\")\n",
    "tf.app.flags.DEFINE_float(\"l2_lambda\", 0.01, \"Hyperparameter for l2 regularization.\")\n",
    "tf.app.flags.DEFINE_float(\"max_gradient_norm\", 3.0, \"Parameter for gradient clipping.\")\n",
    "tf.app.flags.DEFINE_string(\"batch_permutation\", \"random\",\n",
    "                           \"Choose whether training data is shuffled ('random'), ordered by length ('by_length'), \"\n",
    "                           \"or kept in initial order ('None') for each epoch\")\n",
    "tf.app.flags.DEFINE_integer(\"decrease_lr\", 0, \"Whether to decrease learning rate lr over time\")\n",
    "tf.app.flags.DEFINE_float(\"lr_d_base\", 0.9997, \"Base for the exponential decay of lr\")\n",
    "tf.app.flags.DEFINE_float(\"lr_divider\", 2, \"Due to exp. decay, lr can get as small as lr/lr_divider but not smaller\")\n",
    "# tf.app.flags.DEFINE_string(\"data_dir\", \"data/squad/\", \"SQuAD data directory\")\n",
    "# tf.app.flags.DEFINE_string(\"data_dir\", \"/content/DCN-Squad-Colab/data/squad_min/\", \"SQuAD data directory\")\n",
    "# tf.app.flags.DEFINE_string(\"glove_dir\", \"/content/DCN-Squad-Colab/data/glove/\", \"Glove and Vocab data directory\")\n",
    "# tf.app.flags.DEFINE_string(\"checkpoint_dir\", \"/gdrive/My Drive/Colab Notebooks/DCN/model/\", \"Tensorflow Chekpoints\")\n",
    "\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", \"data/squad_min/\", \"SQuAD data directory\")\n",
    "tf.app.flags.DEFINE_string(\"glove_dir\", \"data/glove/\", \"Glove and Vocab data directory\")\n",
    "tf.app.flags.DEFINE_string(\"checkpoint_dir\", \"model/\", \"Tensorflow Chekpoints\")\n",
    "tf.app.flags.DEFINE_string(\"log_dir\", \"logs/\", \"Tensorboard Logs\")\n",
    "\n",
    "self_max_q_length = 30\n",
    "self_max_c_length = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def span_to_y(y, max_length=None):\n",
    "    \"\"\"y is a numpy array, where each row consists of two ints start_id and end_id. \n",
    "    Do a one hot encoding of the start_id, and another one for the end_id.\n",
    "    @max_length is the length of a context paragraph. Hence the one hot vectors have length @max_length\"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = self_max_c_length\n",
    "    start_ids, end_ids = y[:, 0], y[:, 1]\n",
    "    S, E = [], []\n",
    "    for i in range(len(start_ids)):\n",
    "        labelS, labelE = np.zeros(max_length, dtype=np.int32), np.zeros(max_length, dtype=np.int32)\n",
    "        if start_ids[i] < max_length and end_ids[i] < max_length:\n",
    "            labelS[start_ids[i]], labelE[end_ids[i]] = 1, 1  # one hot encoding\n",
    "        E.append(labelE)\n",
    "        S.append(labelS)\n",
    "    return np.array(S), np.array(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def read_and_pad(filename, length, pad_value):\n",
    "    \"\"\"filename is a file with words ids. Each row can have a different amount of ids.\n",
    "    Read and pad each row with @pad_value such that it has length @length. \n",
    "    Additionally create a boolean mask for each row. An element is False iff the corresponding id is a pad_value\n",
    "    Returns the padded id array, and the boolean mask.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.split() for line in lines]\n",
    "    line_array, mask_array = [], []\n",
    "    for line in lines:\n",
    "        line = line[:length]  # TODO: Add code to get rid of data for which len(line)>length.\n",
    "        # Note: If the context for one line is not taken, question and answer_span should also not be taken and\n",
    "        # vice versa for the question. Priority not too high, because having 0.1% garbage data is not too bad\n",
    "        add_length = length - len(line)\n",
    "        mask = [True] * len(line) + add_length * [False]\n",
    "        line = line + add_length * [pad_value]\n",
    "        line_array.append(line)\n",
    "        mask_array.append(mask)\n",
    "    return np.array(line_array, dtype=np.int32), np.array(mask_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# def load_and_preprocess_data(self):\n",
    "\"\"\"Read in the Word embedding matrix as well as the question and context paragraphs and bring them into the \n",
    "desired numerical shape.\"\"\"\n",
    "\n",
    "logging.info(\"Data preparation. This can take some seconds...\")\n",
    "# load vocab\n",
    "with open(self_FLAGS.glove_dir + \"vocab.dat\", \"r\") as f:\n",
    "    self_vocab = f.readlines()\n",
    "self_vocab = [x[:-1] for x in self_vocab]\n",
    "# load word embedding\n",
    "if self_FLAGS.word_vec_dim == 300:\n",
    "    self_WordEmbeddingMatrix = np.load(self_FLAGS.glove_dir + \"glove.trimmed.300.npz\")['glove']\n",
    "elif self_FLAGS.word_vec_dim == 100:\n",
    "    self_WordEmbeddingMatrix = np.load(self_FLAGS.glove_dir + \"glove.trimmed.100.npz\")['glove']\n",
    "else:\n",
    "    raise ValueError(\"word_vec_dim can be either 100 or 300\")\n",
    "logging.debug(\"WordEmbeddingMatrix.shape={}\".format(self_WordEmbeddingMatrix.shape))\n",
    "null_wordvec_index = self_WordEmbeddingMatrix.shape[0]\n",
    "# append a zero vector to WordEmbeddingMatrix, which shall be used as padding value\n",
    "self_WordEmbeddingMatrix = np.vstack((self_WordEmbeddingMatrix, np.zeros(self_FLAGS.word_vec_dim)))\n",
    "self_WordEmbeddingMatrix = self_WordEmbeddingMatrix.astype(np.float32)\n",
    "logging.debug(\"WordEmbeddingMatrix.shape after appending zero vector={}\".format(self_WordEmbeddingMatrix.shape))\n",
    "\n",
    "# load contexts, questions and labels\n",
    "self_yS, self_yE = span_to_y(np.loadtxt(self_FLAGS.data_dir + \"train.span\", dtype=np.int32))\n",
    "self_yvalS, self_yvalE = span_to_y(np.loadtxt(self_FLAGS.data_dir + \"val.span\", dtype=np.int32))\n",
    "\n",
    "self_X_c, self_X_c_mask = read_and_pad(self_FLAGS.data_dir + \"train.ids.context\", self_max_c_length,\n",
    "                                            null_wordvec_index)\n",
    "self_Xval_c, self_Xval_c_mask = read_and_pad(self_FLAGS.data_dir + \"val.ids.context\", self_max_c_length,\n",
    "                                                  null_wordvec_index)\n",
    "self_X_q, self_X_q_mask = read_and_pad(self_FLAGS.data_dir + \"train.ids.question\", self_max_q_length,\n",
    "                                            null_wordvec_index)\n",
    "self_Xval_q, self_Xval_q_mask = read_and_pad(self_FLAGS.data_dir + \"val.ids.question\", self_max_q_length,\n",
    "                                                  null_wordvec_index)\n",
    "\n",
    "logging.info(\"End data preparation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "self_q_input_placeholder = tf.placeholder(tf.int32, (None, self_max_q_length), name=\"q_input_ph\")\n",
    "self_q_mask_placeholder = tf.placeholder(dtype=tf.bool, shape=(None, self_max_q_length),\n",
    "                                         name=\"q_mask_placeholder\")\n",
    "self_c_input_placeholder = tf.placeholder(tf.int32, (None, self_max_c_length), name=\"c_input_ph\")\n",
    "self_c_mask_placeholder = tf.placeholder(dtype=tf.bool, shape=(None, self_max_c_length),\n",
    "                                         name=\"c_mask_placeholder\")\n",
    "self_labels_placeholderS = tf.placeholder(tf.int32, (None, self_max_c_length), name=\"label_phS\")\n",
    "self_labels_placeholderE = tf.placeholder(tf.int32, (None, self_max_c_length), name=\"label_phE\")\n",
    "\n",
    "self_dropout_placeholder = tf.placeholder(tf.float32, name=\"dropout_ph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: Encode / Decode Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def encode(apply_dropout=False):\n",
    "    \"\"\"Coattention context encoder as introduced in https://arxiv.org/abs/1611.01604 \n",
    "    Uses GRUs instead of LSTMs. \"\"\"\n",
    "\n",
    "    # Each word is represented by a glove word vector (https://nlp.stanford.edu/projects/glove/)\n",
    "    self_WEM = tf.get_variable(name=\"WordEmbeddingMatrix\", initializer=tf.constant(self_WordEmbeddingMatrix),\n",
    "                               trainable=False)\n",
    "\n",
    "    # map word index (integer) to word vector (100 dimensional float vector)\n",
    "    self_embedded_q = tf.nn.embedding_lookup(params=self_WEM, ids=self_q_input_placeholder)\n",
    "    self_embedded_c = tf.nn.embedding_lookup(params=self_WEM, ids=self_c_input_placeholder)\n",
    "\n",
    "    rnn_size = self_FLAGS.rnn_state_size\n",
    "    with tf.variable_scope(\"rnn\", reuse=None):\n",
    "        cell = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "        if apply_dropout:\n",
    "            # TODO add separate dropout placeholder for encoding and decoding. Right now the maximum sets\n",
    "            # enc_keep_prob to 1 during prediction.\n",
    "            enc_keep_prob = tf.maximum(tf.constant(self_FLAGS.dropout_encoder), self_dropout_placeholder)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=enc_keep_prob)\n",
    "        q_sequence_length = tf.reduce_sum(tf.cast(self_q_mask_placeholder, tf.int32), axis=1)\n",
    "        q_sequence_length = tf.reshape(q_sequence_length, [-1, ])\n",
    "\n",
    "        q_outputs, q_final_state = tf.nn.dynamic_rnn(cell=cell, inputs=self_embedded_q,\n",
    "                                                     sequence_length=q_sequence_length, dtype=tf.float32,\n",
    "                                                     time_major=False)\n",
    "\n",
    "    Qprime = q_outputs\n",
    "    q_senti = tf.get_variable(\"q_senti0\", (rnn_size,), dtype=tf.float32)\n",
    "    q_senti = tf.tile(q_senti, tf.shape(Qprime)[0:1])\n",
    "    q_senti = tf.reshape(q_senti, (-1, 1, tf.shape(Qprime)[2]))\n",
    "    Qprime = tf.concat([Qprime, q_senti], axis=1)\n",
    "    Qprime = tf.transpose(Qprime, [0, 2, 1], name=\"Qprime\")\n",
    "\n",
    "    # add tanh layer to go from Qprime to Q\n",
    "    WQ = tf.get_variable(\"WQ\", (self_max_q_length + 1, self_max_q_length + 1),\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    bQ = tf.get_variable(\"bQ_Bias\", shape=(rnn_size, self_max_q_length + 1),\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Q = tf.einsum('ijk,kl->ijl', Qprime, WQ)\n",
    "    Q = tf.nn.tanh(Q + bQ, name=\"Q\")\n",
    "\n",
    "    with tf.variable_scope(\"rnn\", reuse=True):\n",
    "        c_sequence_length = tf.reduce_sum(tf.cast(self_c_mask_placeholder, tf.int32), axis=1)\n",
    "        c_sequence_length = tf.reshape(c_sequence_length, [-1, ])\n",
    "        # use the same RNN cell as for the question input\n",
    "        c_outputs, c_final_state = tf.nn.dynamic_rnn(cell=cell, inputs=self_embedded_c,\n",
    "                                                     sequence_length=c_sequence_length,\n",
    "                                                     dtype=tf.float32,\n",
    "                                                     time_major=False)\n",
    "\n",
    "    D = c_outputs\n",
    "    c_senti = tf.get_variable(\"c_senti0\", (rnn_size,), dtype=tf.float32)\n",
    "    c_senti = tf.tile(c_senti, tf.shape(D)[0:1])\n",
    "    c_senti = tf.reshape(c_senti, (-1, 1, tf.shape(D)[2]))\n",
    "    D = tf.concat([D, c_senti], axis=1)\n",
    "    D = tf.transpose(D, [0, 2, 1])\n",
    "    L = tf.einsum('ijk,ijl->ikl', D, Q)\n",
    "    AQ = tf.nn.softmax(L)\n",
    "    AD = tf.nn.softmax(tf.transpose(L, [0, 2, 1]))\n",
    "    CQ = tf.matmul(D, AQ)\n",
    "    CD1 = tf.matmul(Q, AD)\n",
    "    CD2 = tf.matmul(CQ, AD)\n",
    "    CD = tf.concat([CD1, CD2], axis=1)\n",
    "    CDprime = tf.concat([CD, D], axis=1)\n",
    "    CDprime = tf.transpose(CDprime, [0, 2, 1])\n",
    "\n",
    "    with tf.variable_scope(\"u_rnn\", reuse=False):\n",
    "        cell_fw = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "        cell_bw = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "        if apply_dropout:\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob=enc_keep_prob)\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob=enc_keep_prob)\n",
    "\n",
    "        (cc_fw, cc_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs=CDprime,\n",
    "                                                            sequence_length=c_sequence_length,\n",
    "                                                            dtype=tf.float32)\n",
    "\n",
    "    U = tf.concat([cc_fw, cc_bw], axis=2)\n",
    "    logging.debug(\"U={}\".format(U))\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def dp_decode_HMN(U, pool_size=4, apply_dropout=True, cumulative_loss=True, apply_l2_reg=False):\n",
    "    \"\"\" input: coattention_context U. tensor of shape (batch_size, context_length, arbitrary)\n",
    "    Implementation of dynamic pointer decoder proposed by Xiong et al. ( https://arxiv.org/abs/1611.01604).\n",
    "\n",
    "    Some of the implementation details such as the way us is obained from U via tf.gather_nd() are explored on toy \n",
    "    data in Experimentation_Notebooks/toy_data_examples_for_tile_map_fn_gather_nd_etc.ipynb\"\"\"\n",
    "\n",
    "    def HMN_func(dim, ps):  # ps=pool size, HMN = highway maxout network\n",
    "        def func(ut, h, us, ue):\n",
    "            h_us_ue = tf.concat([h, us, ue], axis=1)\n",
    "            WD = tf.get_variable(name=\"WD\", shape=(5 * dim, dim), dtype='float32',\n",
    "                                 initializer=xavier_initializer())\n",
    "            r = tf.nn.tanh(tf.matmul(h_us_ue, WD))\n",
    "            ut_r = tf.concat([ut, r], axis=1)\n",
    "            if apply_dropout:\n",
    "                ut_r = tf.nn.dropout(ut_r, keep_prob=self_dropout_placeholder)\n",
    "            W1 = tf.get_variable(name=\"W1\", shape=(3 * dim, dim, ps), dtype='float32',\n",
    "                                 initializer=xavier_initializer())\n",
    "            b1 = tf.get_variable(name=\"b1_Bias\", shape=(dim, ps), dtype='float32',\n",
    "                                 initializer=tf.zeros_initializer())\n",
    "            mt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1\n",
    "            mt1 = tf.reduce_max(mt1, axis=2)\n",
    "            if apply_dropout:\n",
    "                mt1 = tf.nn.dropout(mt1, self_dropout_placeholder)\n",
    "            W2 = tf.get_variable(name=\"W2\", shape=(dim, dim, ps), dtype='float32',\n",
    "                                 initializer=xavier_initializer())\n",
    "            b2 = tf.get_variable(name=\"b2_Bias\", shape=(dim, ps), dtype='float32',\n",
    "                                 initializer=tf.zeros_initializer())\n",
    "            mt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2\n",
    "            mt2 = tf.reduce_max(mt2, axis=2)\n",
    "            mt12 = tf.concat([mt1, mt2], axis=1)\n",
    "            if apply_dropout:\n",
    "                mt12 = tf.nn.dropout(mt12, keep_prob=self_dropout_placeholder)\n",
    "            W3 = tf.get_variable(name=\"W3\", shape=(2 * dim, 1, ps), dtype='float32',\n",
    "                                 initializer=xavier_initializer())\n",
    "            b3 = tf.get_variable(name=\"b3_Bias\", shape=(1, ps), dtype='float32', initializer=tf.zeros_initializer())\n",
    "            hmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3\n",
    "            hmn = tf.reduce_max(hmn, axis=2)\n",
    "            hmn = tf.reshape(hmn, [-1])\n",
    "            return hmn\n",
    "\n",
    "        return func\n",
    "\n",
    "    float_mask = tf.cast(self_c_mask_placeholder, dtype=tf.float32)\n",
    "    neg = tf.constant([0], dtype=tf.float32)\n",
    "    neg = tf.tile(neg, [tf.shape(float_mask)[0]])\n",
    "    neg = tf.reshape(neg, (tf.shape(float_mask)[0], 1))\n",
    "    float_mask = tf.concat([float_mask, neg], axis=1)\n",
    "    labels_S = tf.concat([self_labels_placeholderS, tf.cast(neg, tf.int32)], axis=1)\n",
    "    labels_E = tf.concat([self_labels_placeholderE, tf.cast(neg, tf.int32)], axis=1)\n",
    "    dim = self_FLAGS.rnn_state_size\n",
    "\n",
    "    # initialize us and ue as first word in context\n",
    "    i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype='int32')\n",
    "    i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype='int32')\n",
    "    idx = tf.range(0, tf.shape(U)[0], 1)\n",
    "    s_idx = tf.stack([idx, i_start], axis=1)\n",
    "    e_idx = tf.stack([idx, i_end], axis=1)\n",
    "    us = tf.gather_nd(U, s_idx)\n",
    "    ue = tf.gather_nd(U, e_idx)\n",
    "\n",
    "    HMN_alpha = HMN_func(dim, pool_size)\n",
    "    HMN_beta = HMN_func(dim, pool_size)\n",
    "\n",
    "    alphas, betas = [], []\n",
    "    h = tf.zeros(shape=(tf.shape(U)[0], dim), dtype='float32', name=\"h_dpd\")  # initial hidden state of RNN\n",
    "    U_transpose = tf.transpose(U, [1, 0, 2])\n",
    "\n",
    "    with tf.variable_scope(\"dpd_RNN\"):\n",
    "        cell = tf.contrib.rnn.GRUCell(dim)\n",
    "        for time_step in range(3):  # number of time steps can be considered as a hyper parameter\n",
    "            if time_step >= 1:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            us_ue = tf.concat([us, ue], axis=1)\n",
    "            _, h = cell(inputs=us_ue, state=h)\n",
    "\n",
    "            with tf.variable_scope(\"alpha_HMN\"):\n",
    "                if time_step >= 1:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                alpha = tf.map_fn(lambda ut: HMN_alpha(ut, h, us, ue), U_transpose, dtype=tf.float32)\n",
    "                alpha = tf.transpose(alpha, [1, 0]) * float_mask\n",
    "\n",
    "            i_start = tf.argmax(alpha, 1)\n",
    "            idx = tf.range(0, tf.shape(U)[0], 1)\n",
    "            s_idx = tf.stack([idx, tf.cast(i_start, 'int32')], axis=1)\n",
    "            us = tf.gather_nd(U, s_idx)\n",
    "\n",
    "            with tf.variable_scope(\"beta_HMN\"):\n",
    "                if time_step >= 1:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                beta = tf.map_fn(lambda ut: HMN_beta(ut, h, us, ue), U_transpose, dtype=tf.float32)\n",
    "                beta = tf.transpose(beta, [1, 0]) * float_mask\n",
    "\n",
    "            i_end = tf.argmax(beta, 1)\n",
    "            e_idx = tf.stack([idx, tf.cast(i_end, 'int32')], axis=1)\n",
    "            ue = tf.gather_nd(U, e_idx)\n",
    "\n",
    "            alphas.append(alpha)\n",
    "            betas.append(beta)\n",
    "\n",
    "    if cumulative_loss:\n",
    "        losses_alpha = [tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels_S, logits=a) for a in\n",
    "                        alphas]\n",
    "        losses_alpha = [tf.reduce_mean(x) for x in losses_alpha]\n",
    "        losses_beta = [tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels_E, logits=b) for b in\n",
    "                       betas]\n",
    "        losses_beta = [tf.reduce_mean(x) for x in losses_beta]\n",
    "\n",
    "        loss = tf.reduce_sum([losses_alpha, losses_beta])\n",
    "    else:\n",
    "        cross_entropy_start = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels_S, logits=alpha,\n",
    "                                                                      name=\"cross_entropy_start\")\n",
    "        cross_entropy_end = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels_E, logits=beta,\n",
    "                                                                    name=\"cross_entropy_end\")\n",
    "        loss = tf.reduce_mean(cross_entropy_start) + tf.reduce_mean(cross_entropy_end)\n",
    "\n",
    "    if apply_l2_reg:\n",
    "        loss_l2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \"Bias\" not in v.name])\n",
    "        loss += loss_l2 * self_FLAGS.l2_lambda\n",
    "\n",
    "    return i_start, i_end, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "coattention_context = encode(apply_dropout=True)\n",
    "self_predictionS, self_predictionE, self_loss = dp_decode_HMN(coattention_context, apply_dropout=True, apply_l2_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "step_adam = tf.Variable(0, trainable=False)\n",
    "lr = tf.constant(self_FLAGS.learning_rate)\n",
    "if self_FLAGS.decrease_lr:\n",
    "    # use adam optimizer with exponentially decaying learning rate\n",
    "    rate_adam = tf.train.exponential_decay(lr, step_adam, 1, self_FLAGS.lr_d_base)\n",
    "    # after one epoch: # 0.999**2500 = 0.5,  hence learning rate decays by a factor of 0.5 each epoch\n",
    "    rate_adam = tf.maximum(rate_adam, tf.constant(self_FLAGS.learning_rate / self_FLAGS.lr_divider))\n",
    "    # should not go down by more than a factor of 2\n",
    "    self_optimizer = tf.train.AdamOptimizer(rate_adam)\n",
    "else:\n",
    "    self_optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "grads_and_vars = self_optimizer.compute_gradients(self_loss)\n",
    "variables = [output[1] for output in grads_and_vars]\n",
    "gradients = [output[0] for output in grads_and_vars]\n",
    "\n",
    "gradients = tf.clip_by_global_norm(gradients, clip_norm=self_FLAGS.max_gradient_norm)[0]\n",
    "self_global_grad_norm = tf.global_norm(gradients)\n",
    "grads_and_vars = [(gradients[i], variables[i]) for i in range(len(gradients))]\n",
    "\n",
    "train_op = self_optimizer.apply_gradients(grads_and_vars, global_step=step_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_feed_dict(batch_xc, batch_xc_mask, batch_xq, batch_xq_mask, batch_yS, batch_yE, keep_prob):\n",
    "    feed_dict = {self_c_input_placeholder: batch_xc,\n",
    "                 self_c_mask_placeholder: batch_xc_mask,\n",
    "                 self_q_input_placeholder: batch_xq,\n",
    "                 self_q_mask_placeholder: batch_xq_mask,\n",
    "                 self_labels_placeholderS: batch_yS,\n",
    "                 self_labels_placeholderE: batch_yE,\n",
    "                 self_dropout_placeholder: keep_prob}\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def squad_normalize_answer(s):\n",
    "    \"\"\" Lower text and remove punctuation, articles and extra whitespace.\n",
    "    Method copied from the SQuAD Leaderboard: https://rajpurkar.github.io/SQuAD-explorer/  \"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def squad_f1_score(prediction, ground_truth):\n",
    "    \"\"\"Method copied from the SQuAD Leaderboard: https://rajpurkar.github.io/SQuAD-explorer/\"\"\"\n",
    "    prediction_tokens = squad_normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = squad_normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def squad_exact_match_score(prediction, ground_truth):\n",
    "    \"\"\"Method copied from the SQuAD Leaderboard: https://rajpurkar.github.io/SQuAD-explorer/\"\"\"\n",
    "    return (squad_normalize_answer(prediction) == squad_normalize_answer(ground_truth))\n",
    "\n",
    "def get_f1(yS, yE, ypS, ypE):\n",
    "    \"\"\"My own, more strict f1 metric\"\"\"\n",
    "    f1_tot = 0.0\n",
    "    for i in range(len(yS)):\n",
    "        y = np.zeros(self_max_c_length)\n",
    "        s = np.argmax(yS[i])\n",
    "        e = np.argmax(yE[i])\n",
    "        y[s:e + 1] = 1\n",
    "\n",
    "        yp = np.zeros_like(y)\n",
    "        yp[ypS[i]:ypE[i] + 1] = 1\n",
    "        yp[ypE[i]:ypS[i] + 1] = 1  # allow flipping between start and end\n",
    "\n",
    "        n_true_pos = np.sum(y * yp)\n",
    "        n_pred_pos = np.sum(yp)\n",
    "        n_actual_pos = np.sum(y)\n",
    "        if n_true_pos != 0:\n",
    "            precision = 1.0 * n_true_pos / n_pred_pos\n",
    "            recall = 1.0 * n_true_pos / n_actual_pos\n",
    "            f1_tot += (2 * precision * recall) / (precision + recall)\n",
    "    f1_tot /= len(yS)\n",
    "    return f1_tot\n",
    "\n",
    "def get_exact_match(yS, yE, ypS, ypE):\n",
    "    \"\"\"My own, more strict EM metric\"\"\"\n",
    "    count = 0\n",
    "    for i in range(len(yS)):\n",
    "        s, e = np.argmax(yS[i]), np.argmax(yE[i])\n",
    "        sp, ep = ypS[i], ypE[i]\n",
    "        if sp > ep:\n",
    "            sp, ep = ep, sp  # allow flipping between start and end\n",
    "        if s == sp and e == ep:\n",
    "            count += 1\n",
    "    match_fraction = count / float(len(yS))\n",
    "    return match_fraction\n",
    "\n",
    "def index_list_to_string(index_list):\n",
    "    \"\"\"Helper function. Converts a list of word indices to a string of words\"\"\"\n",
    "    res = [self_vocab[index] for index in index_list]\n",
    "    return ' '.join(res)\n",
    "\n",
    "def get_exact_match_from_tokens(yS, yE, ypS, ypE, batch_Xc):\n",
    "    \"\"\"This function doesn't compare the indices, but the tokens behind the indices. This is a bit more forgiving\n",
    "    and it is the metric applied on the SQuAD leaderboard\"\"\"\n",
    "    em = 0\n",
    "    for i in range(len(yS)):\n",
    "        s, e = np.argmax(yS[i]), np.argmax(yE[i])\n",
    "        sp, ep = ypS[i], ypE[i]\n",
    "        if sp > ep:\n",
    "            sp, ep = ep, sp  # allow flipping between start and end\n",
    "        ground_truth = index_list_to_string(batch_Xc[i][s:e + 1])\n",
    "        prediction = index_list_to_string(batch_Xc[i][sp:ep + 1])\n",
    "        em += squad_exact_match_score(prediction, ground_truth)\n",
    "    return em / float(len(yS))\n",
    "\n",
    "def get_f1_from_tokens(yS, yE, ypS, ypE, batch_Xc):\n",
    "    \"\"\"This function doesn't compare the indices, but the tokens behind the indices. This is a bit more forgiving\n",
    "    and it is the metric applied on the SQuAD leaderboard.\"\"\"\n",
    "    f1 = 0\n",
    "    for i in range(len(yS)):\n",
    "        s, e = np.argmax(yS[i]), np.argmax(yE[i])\n",
    "        sp, ep = ypS[i], ypE[i]\n",
    "        if sp > ep:\n",
    "            sp, ep = ep, sp  # allow flipping between start and end\n",
    "        ground_truth = index_list_to_string(batch_Xc[i][s:e + 1])\n",
    "        prediction = index_list_to_string(batch_Xc[i][sp:ep + 1])\n",
    "        f1 += squad_f1_score(prediction, ground_truth)\n",
    "    return f1 / float(len(yS))\n",
    "\n",
    "def plot_metrics(prefix, index_epoch, losses, val_losses, EMs, val_Ems, F1s, val_F1s, grad_norms):\n",
    "    n_data_points = len(losses)\n",
    "    epoch_axis = np.arange(n_data_points, dtype=np.float32) * index_epoch / float(n_data_points)\n",
    "    epoch_axis_val = list(range(index_epoch + 1))\n",
    "\n",
    "    plt.plot(epoch_axis, losses, label=\"training\")\n",
    "    plt.plot(epoch_axis_val, [losses[0]] + val_losses, label=\"validation\", marker=\"x\", ms=15)\n",
    "    # initial value is from training set, just so that we have a nice value for epoch=0 in the plot\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(self_FLAGS.figure_directory + prefix + \"losses_over_time.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(epoch_axis, EMs, label=\"training\")\n",
    "    plt.plot(epoch_axis_val, [EMs[0]] + val_Ems, label=\"validation\", marker=\"x\", ms=15)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"EM\")\n",
    "    plt.legend()\n",
    "    plt.savefig(self_FLAGS.figure_directory + prefix + \"EMs_over_time.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(epoch_axis, F1s, label=\"training\")\n",
    "    plt.plot(epoch_axis_val, [F1s[0]] + val_F1s, label=\"validation\", marker=\"x\", ms=15)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"F1\")\n",
    "    plt.legend()\n",
    "    plt.savefig(self_FLAGS.figure_directory + prefix + \"f1s_over_time.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(epoch_axis, grad_norms)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"gradient_norm\")\n",
    "    plt.savefig(self_FLAGS.figure_directory + prefix + \"training_grad_norms_over_time.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class batch(object):\n",
    "    def __init__(self, X_c, X_c_mask, X_q, X_q_mask, yS, yE, Xval_c, Xval_c_mask, Xval_q, Xval_q_mask, yvalS, yvalE):\n",
    "        self.X_c = X_c\n",
    "        self.X_c_mask = X_c_mask\n",
    "        self.X_q = X_q \n",
    "        self.X_q_mask = X_q_mask \n",
    "        self.yS = yS \n",
    "        self.yE = yE \n",
    "        self.Xval_c = Xval_c \n",
    "        self.Xval_c_mask = Xval_c_mask \n",
    "        self.Xval_q = Xval_q \n",
    "        self.Xval_q_mask = Xval_q_mask \n",
    "        self.yvalS = yvalS \n",
    "        self.yvalE = yvalE\n",
    "        self.batch_index = 0\n",
    "        self.max_batch_index = -1\n",
    "        self.batch_permutation = []\n",
    "    \n",
    "    def initialize_batch_processing(self, permutation='None', n_samples=None):\n",
    "        self.batch_index = 0\n",
    "        if n_samples is not None:\n",
    "            self.max_batch_index = n_samples\n",
    "        if permutation == 'by_length':\n",
    "            # sum over True/False gives number of words in each sample\n",
    "            length_of_each_context_paragraph = np.sum(self.X_c_mask, axis=1)\n",
    "            # permutation of data is chosen, such that the algorithm sees short context_paragraphs first\n",
    "            self.batch_permutation = np.argsort(length_of_each_context_paragraph)\n",
    "        elif permutation == 'random':\n",
    "            self.batch_permutation = np.random.permutation(self.max_batch_index)  # random initial permutation\n",
    "        elif (permutation == 'None' or permutation is None):  # no permutation\n",
    "            self.batch_permutation = np.arange(self.max_batch_index)  # initial permutation = identity\n",
    "        else:\n",
    "            raise ValueError(\"permutation must be 'by_length', 'random' or 'None'\")\n",
    "\n",
    "    def next_batch(self, batch_size, permutation_after_epoch='None', val=False):\n",
    "        if self.batch_index >= self.max_batch_index:\n",
    "            # we went through one epoch. reset batch_index and initialize batch_permutation\n",
    "            self.initialize_batch_processing(permutation=permutation_after_epoch)\n",
    "\n",
    "        start = self.batch_index\n",
    "        end = self.batch_index + batch_size\n",
    "\n",
    "        if not val:\n",
    "            Xcres = self.X_c[self.batch_permutation[start:end]]\n",
    "            Xcmaskres = self.X_c_mask[self.batch_permutation[start:end]]\n",
    "            Xqres = self.X_q[self.batch_permutation[start:end]]\n",
    "            Xqmaskres = self.X_q_mask[self.batch_permutation[start:end]]\n",
    "            yresS = self.yS[self.batch_permutation[start:end]]\n",
    "            yresE = self.yE[self.batch_permutation[start:end]]\n",
    "        else:\n",
    "            Xcres = self.Xval_c[self.batch_permutation[start:end]]\n",
    "            Xcmaskres = self.Xval_c_mask[self.batch_permutation[start:end]]\n",
    "            Xqres = self.Xval_q[self.batch_permutation[start:end]]\n",
    "            Xqmaskres = self.Xval_q_mask[self.batch_permutation[start:end]]\n",
    "            yresS = self.yvalS[self.batch_permutation[start:end]]\n",
    "            yresE = self.yvalE[self.batch_permutation[start:end]]\n",
    "\n",
    "        self.batch_index += batch_size\n",
    "        return Xcres, Xcmaskres, Xqres, Xqmaskres, yresS, yresE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 14/31 [03:47<04:39, 16.43s/it, loss=29.5, EM=0, SQ_EM=0, F1=0.028, SQ_F1=0.0447, grad_norm=3, lr=0.001]     "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-18fedbcaef63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m             [train_op, self_loss, self_predictionS, self_predictionE, self_global_grad_norm,\n\u001b[1;32m     62\u001b[0m              self_optimizer._lr],\n\u001b[0;32m---> 63\u001b[0;31m             feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_exact_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_yS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_yE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mf1s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_yS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_yE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.name_scope('Performance'):\n",
    "    tb_f1 = tf.placeholder(tf.float32, shape=None, name='f1_summary')\n",
    "    tb_em = tf.placeholder(tf.float32, shape=None, name='em_summary')\n",
    "    tb_loss = tf.placeholder(tf.float32, shape=None, name='loss_summary')\n",
    "\n",
    "    tf.summary.scalar('F1', tb_f1)\n",
    "    tf.summary.scalar('EM', tb_em)\n",
    "    tf.summary.scalar('Loss', tb_loss)\n",
    "    tb_step = tf.Variable(0, tf.int32)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epochs = self_FLAGS.epochs\n",
    "batch_size = self_FLAGS.batch_size\n",
    "n_samples = len(self_yS)\n",
    "\n",
    "ba = batch(self_X_c, self_X_c_mask, self_X_q, self_X_q_mask, self_yS, self_yE, self_Xval_c, self_Xval_c_mask, self_Xval_q, self_Xval_q_mask, self_yvalS, self_yvalE)\n",
    "\n",
    "global_losses, global_EMs, global_f1s, global_grad_norms = [], [], [], []  # global means \"over several epochs\"\n",
    "EMs_val, F1s_val, loss_val = [], [], []  # exact_match- and F1-metrics as well as loss on the validation data\n",
    "SQ_global_EMs, SQ_global_f1s, SQ_EMs_val, SQ_F1s_val = [], [], [], []  # corresponding squad metrics\n",
    "\n",
    "\n",
    "########### LOGIN IN TENSORBOARD #####################\n",
    "# with tf.name_scope('Paper'):\n",
    "# #     f1_summary = tf.summary.scalar(\"F1\", np.mean(f1s))\n",
    "# #     em_summary = tf.summary.scalar(\"EM\", np.mean(ems))\n",
    "#     loss_summary = tf.summary.scalar(\"loss\", self_loss)\n",
    "\n",
    "# # with tf.name_scope('Squad'):\n",
    "# #     f1_summary_s = tf.summary.scalar(\"Squad_F1\", np.mean(sq_f1s))\n",
    "# #     em_summary_s = tf.summary.scalar(\"Squad_EM\", np.mean(sq_ems))\n",
    "\n",
    "writer = tf.summary.FileWriter(self_FLAGS.log_dir, sess.graph)\n",
    "merged_op = tf.summary.merge_all()\n",
    "tb_step_op = tb_step.assign_add(1)\n",
    "\n",
    "\n",
    "\n",
    "for index_epoch in range(1, epochs + 1):\n",
    "    progbar = trange(int(n_samples / batch_size))\n",
    "    losses, ems, f1s, grad_norms = [], [], [], []\n",
    "    sq_ems, sq_f1s = [], []\n",
    "    ba.initialize_batch_processing(permutation=self_FLAGS.batch_permutation, n_samples=n_samples)\n",
    "\n",
    "    ############### train for one epoch ###############\n",
    "    for _ in progbar:\n",
    "        batch_xc, batch_xc_mask, batch_xq, batch_xq_mask, batch_yS, batch_yE = ba.next_batch(\n",
    "            batch_size=batch_size, permutation_after_epoch=self_FLAGS.batch_permutation)\n",
    "        feed_dict = get_feed_dict(batch_xc, batch_xc_mask, batch_xq, batch_xq_mask, batch_yS, batch_yE,\n",
    "                                       self_FLAGS.dropout)\n",
    "        _, current_loss, predictionS, predictionE, grad_norm, curr_lr = sess.run(\n",
    "            [train_op, self_loss, self_predictionS, self_predictionE, self_global_grad_norm,\n",
    "             self_optimizer._lr],\n",
    "            feed_dict=feed_dict)\n",
    "        ems.append(get_exact_match(batch_yS, batch_yE, predictionS, predictionE))\n",
    "        f1s.append(get_f1(batch_yS, batch_yE, predictionS, predictionE))\n",
    "        sq_ems.append(get_exact_match_from_tokens(batch_yS, batch_yE, predictionS, predictionE, batch_xc))\n",
    "        sq_f1s.append(get_f1_from_tokens(batch_yS, batch_yE, predictionS, predictionE, batch_xc))\n",
    "        losses.append(current_loss)\n",
    "        grad_norms.append(grad_norm)\n",
    "\n",
    "        if len(losses) >= 20:\n",
    "            progbar.set_postfix({'loss': np.mean(losses), 'EM': np.mean(ems), 'SQ_EM': np.mean(sq_ems), 'F1':\n",
    "                np.mean(f1s), 'SQ_F1': np.mean(sq_f1s), 'grad_norm': np.mean(grad_norms), 'lr': curr_lr})\n",
    "            global_losses.append(np.mean(losses))\n",
    "            global_EMs.append(np.mean(ems))\n",
    "            global_f1s.append(np.mean(f1s))\n",
    "            SQ_global_EMs.append(np.mean(sq_ems))\n",
    "            SQ_global_f1s.append(np.mean(sq_f1s))\n",
    "            global_grad_norms.append(np.mean(grad_norms))\n",
    "            \n",
    "            summary_train = sess.run(merged_op, feed_dict={tb_f1: np.mean(f1s), \n",
    "                                                           tb_em: np.mean(ems),\n",
    "                                                           tb_loss: np.mean(losses)})\n",
    "            counter = sess.run(tb_step_op)\n",
    "            writer.add_summary(summary_train, counter)\n",
    "\n",
    "            losses, ems, f1s, grad_norms = [], [], [], []\n",
    "            sq_ems, sq_f1s = [], []\n",
    "\n",
    "\n",
    "\n",
    "    ############## SAVING CHECKPOINT MODEL ###################################\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, self_FLAGS.checkpoint_dir+\"model.ckpt\")\n",
    "    logging.info(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "\n",
    "    ############### After an epoch: evaluate on validation set ###############\n",
    "    logging.info(\"Epoch {} finished. Doing evaluation on validation set...\".format(index_epoch))\n",
    "    ba.initialize_batch_processing(permutation=self_FLAGS.batch_permutation,\n",
    "                                     n_samples=len(self_yvalE))\n",
    "    val_batch_size = batch_size  # can be a multiple of batch_size, but be sure to not run out of memory\n",
    "    losses, ems, f1s = [], [], []\n",
    "    sq_ems, sq_f1s = [], []\n",
    "    for _ in range(int(len(self_yvalE) / val_batch_size)):\n",
    "        batch_xc, batch_xc_mask, batch_xq, batch_xq_mask, batch_yS, batch_yE = ba.next_batch(\n",
    "            batch_size=val_batch_size, permutation_after_epoch=self_FLAGS.batch_permutation, val=True)\n",
    "        feed_dict = get_feed_dict(batch_xc, batch_xc_mask, batch_xq, batch_xq_mask, batch_yS,\n",
    "                                       batch_yE, keep_prob=1)\n",
    "        current_loss, predictionS, predictionE = sess.run([self_loss, self_predictionS, self_predictionE],\n",
    "                                                          feed_dict=feed_dict)\n",
    "        ems.append(get_exact_match(batch_yS, batch_yE, predictionS, predictionE))\n",
    "        f1s.append(get_f1(batch_yS, batch_yE, predictionS, predictionE))\n",
    "        sq_ems.append(get_exact_match_from_tokens(batch_yS, batch_yE, predictionS, predictionE, batch_xc))\n",
    "        sq_f1s.append(get_f1_from_tokens(batch_yS, batch_yE, predictionS, predictionE, batch_xc))\n",
    "        losses.append(current_loss)\n",
    "\n",
    "    loss_on_validation, EM_val, F1_val = np.mean(losses), np.mean(ems), np.mean(f1s)\n",
    "    SQ_F1_val, SQ_EM_val = np.mean(sq_f1s), np.mean(sq_ems)\n",
    "    logging.info(\"loss_val={}\".format(loss_on_validation))\n",
    "    logging.info(\"EM_val={}\".format(EM_val))\n",
    "    logging.info(\"F1_val={}\".format(F1_val))\n",
    "    logging.info(\"SQ_EM_val={}\".format(SQ_EM_val))\n",
    "    logging.info(\"SQ_F1_val={}\".format(SQ_F1_val))\n",
    "    EMs_val.append(EM_val)\n",
    "    F1s_val.append(F1_val)\n",
    "    SQ_EMs_val.append(SQ_EM_val)\n",
    "    SQ_F1s_val.append(SQ_F1_val)\n",
    "    loss_val.append(loss_on_validation)\n",
    "\n",
    "    ############### do some plotting ###############\n",
    "    # if index_epoch > 1:\n",
    "    plot_metrics(\"strict_\", index_epoch, global_losses, loss_val, global_EMs, EMs_val, global_f1s, F1s_val,\n",
    "                      global_grad_norms)\n",
    "\n",
    "    plot_metrics(\"SQuAD_\", index_epoch, global_losses, loss_val, SQ_global_EMs, SQ_EMs_val, SQ_global_f1s,\n",
    "                      SQ_F1s_val, global_grad_norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_id = 1\n",
    "context = ' '.join([self_vocab[i] for i in batch_xc[q_id] if i < len(self_vocab)])\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = ' '.join([self_vocab[i] for i in batch_xq[q_id] if i < len(self_vocab)])\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = np.argmax(batch_yS[q_id])\n",
    "end = np.argmax(batch_yE[q_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = [batch_xc[q_id,i] for i in range(start,end+1)]\n",
    "answer = ' '.join([self_vocab[i] for i in answer if i < len(self_vocab)])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_id = 1\n",
    "feed_dict = get_feed_dict(batch_xc[q_id].reshape(1,-1), \n",
    "                          batch_xc_mask[q_id].reshape(1,-1), \n",
    "                          batch_xq[q_id].reshape(1,-1), \n",
    "                          batch_xq_mask[q_id].reshape(1,-1), \n",
    "                          batch_yS[q_id].reshape(1,-1), \n",
    "                          batch_yE[q_id].reshape(1,-1)\n",
    "                          , keep_prob=1)\n",
    "predictionS, predictionE = sess.run([self_predictionS, self_predictionE], feed_dict=feed_dict)\n",
    "\n",
    "print('Context')\n",
    "context = ' '.join([self_vocab[i] for i in batch_xc[q_id] if i < len(self_vocab)])\n",
    "print(context)\n",
    "\n",
    "print('\\nQuestion')\n",
    "question = ' '.join([self_vocab[i] for i in batch_xq[q_id] if i < len(self_vocab)])\n",
    "print(question)\n",
    "\n",
    "print('\\nPredicted')\n",
    "answer = [batch_xc[q_id,i] for i in range(predictionS[0], predictionE[0]+1)]\n",
    "answer = ' '.join([self_vocab[i] for i in answer if i < len(self_vocab)])\n",
    "print(answer)\n",
    "\n",
    "print('\\nCorrected')\n",
    "start = np.argmax(batch_yS[q_id])\n",
    "end = np.argmax(batch_yE[q_id])\n",
    "answer = [batch_xc[q_id,i] for i in range(start, end+1)]\n",
    "answer = ' '.join([self_vocab[i] for i in answer if i < len(self_vocab)])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
